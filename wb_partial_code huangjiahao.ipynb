{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151562a3",
   "metadata": {},
   "source": [
    "## Natrual language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dbe60a",
   "metadata": {},
   "source": [
    "### (17 points) Word embedding (Skip-gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b73882e",
   "metadata": {},
   "source": [
    "NLTK includes a small selection of texts from the Project Gutenberg electronic text archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/. We begin by getting the Python interpreter to load the NLTK package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13cdfb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90ac787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Shawn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shawn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f4447d",
   "metadata": {},
   "source": [
    "Then we will use the skip-gram algorithm to analyze the first 100 sentences of \"melville-moby_dick.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "090999d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592\n"
     ]
    }
   ],
   "source": [
    "corpus_ = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:100]\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus_]\n",
    "word_count = Counter(flatten(corpus))\n",
    "print(len(word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9355b",
   "metadata": {},
   "source": [
    "**(1) Define the first 1% of the most common words and the first 1% of the least common words as stop words. (2 points)**\n",
    "(Hint: please refer to the most_common() method of Counter class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b884ad82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', 'the', 'of', 'and', 'state', '--(', 'civitas', 'artificial', 'man']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(word_count)\n",
    "\n",
    "word_count.most_common(n)\n",
    "\n",
    "max_idx = int(0.01*n)\n",
    "min_idx = int(0.99*n)+1\n",
    "\n",
    "most_common = word_count.most_common(n)[:max_idx]\n",
    "least_common = word_count.most_common(n)[min_idx:]\n",
    "\n",
    "stopwords = []\n",
    "\n",
    "for item in most_common:\n",
    "    stopwords.append(item[0])\n",
    "    \n",
    "for item in least_common:\n",
    "    stopwords.append(item[0])\n",
    "    \n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f759d",
   "metadata": {},
   "source": [
    "Check the answer: [',', '.', 'the', 'of', 'and', 'man', 'artificial', 'civitas', '--(', 'state']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75302aab",
   "metadata": {},
   "source": [
    "**(2) Construct the vocabulary (1 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf4ad4",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "- You should remove the stop words from the corpus\n",
    "- You should add \\<'UNK'\\> to the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df92f6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582 583\n"
     ]
    }
   ],
   "source": [
    "while len(set(flatten(corpus)))!=582:\n",
    "    for item in corpus:\n",
    "        for word in stopwords:\n",
    "            try:\n",
    "                item.remove(word)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "vocab = list(set(flatten(corpus)))\n",
    "vocab.append('<UNK>')\n",
    "\n",
    "print(len(set(flatten(corpus))), len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62702634",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(vocab) == 583"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c46b6b",
   "metadata": {},
   "source": [
    "**(3) Building the training data  (8 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c0ccd1",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "- Take all pairs containing the central word as training data\n",
    "- Any pair contains '\\<DUMMY\\>' should be ignored\n",
    "\n",
    "**Example**:\n",
    "\n",
    "For window:\n",
    "\n",
    "('\\<DUMMY\\>', '[', 'moby', 'dick', 'by', 'herman', 'melville')\n",
    "\n",
    "We know 'dick' is the central word, so we should capture the following pairs:\n",
    "    \n",
    "('dick', '['), (,'dick','moby') ('dick', 'by') ('dick', 'herman'), ('dick','melville')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a9f1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cc3c7aa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15284\\2071903707.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_training_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWINDOW_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m7606\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE = 3\n",
    "windows = flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1)) for c in corpus])\n",
    "train_data = []\n",
    "def get_training_data(windows, win_size):\n",
    "    for window in windows:\n",
    "        pairs = list(combinations(window,2))\n",
    "        central = window[3]\n",
    "        for pair in pairs:\n",
    "            if '<DUMMY>' not in pair and central in pair:\n",
    "                train_data.append([central,pair])\n",
    "    return train_data\n",
    "    \n",
    "train_data = get_training_data(windows, WINDOW_SIZE)\n",
    "assert len(train_data) == 7606"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0591f08",
   "metadata": {},
   "source": [
    "Now there are many words stored in the training data, and we need to convert them into their indices in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "24d74e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {'<UNK>' : 0}\n",
    "\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "def convert_training_data_into_idx(train_data):\n",
    "    central_word = []\n",
    "    window_word = []\n",
    "    \n",
    "    for data in train_data:\n",
    "        central_word.append(word2index[data[0]])\n",
    "        for win_word in data[1]:\n",
    "            if win_word != data[0]:\n",
    "                window_word.append(word2index[win_word])\n",
    "                \n",
    "    return central_word, window_word\n",
    "central_word, window_word = convert_training_data_into_idx(train_data)\n",
    "train_data = list(zip(central_word, window_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d4ee2",
   "metadata": {},
   "source": [
    "**(4) Build skip-gram model (3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db42003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, projection_dim):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, projection_dim)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, projection_dim)\n",
    "        self.embedding_v.weight.data.uniform_(-1, 1)\n",
    "        self.embedding_u.weight.data.uniform_(0, 0)\n",
    "\n",
    "    def forward(self, center_words,target_words, outer_words):\n",
    "        '''Please return negative log likelihood'''\n",
    "        embed_u = self.u_embeddings(center_words)\n",
    "        embed_v = self.v_embeddings(target_words)\n",
    "\n",
    "        score  = torch.mul(embed_u, embed_v)\n",
    "        score = torch.sum(score, dim=1)\n",
    "        log_target = F.logsigmoid(score).squeeze()\n",
    "\n",
    "        neg_embed_v = self.v_embeddings(outer_words)\n",
    "\n",
    "        neg_score = torch.bmm(neg_embed_v, embed_u.unsqueeze(2)).squeeze()\n",
    "        neg_score = torch.sum(neg_score, dim=1)\n",
    "        sum_log_sampled = F.logsigmoid(-1*neg_score).squeeze()\n",
    "\n",
    "        loss = log_target + sum_log_sampled\n",
    "\n",
    "        return -1*loss.sum()\n",
    "    \n",
    "    def prediction(self, inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17610864",
   "metadata": {},
   "source": [
    "**(5) Train skip-gram model (3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))\n",
    "\n",
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb50bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs, model, batch_size, train_data, opt_fn):\n",
    "    \"\"\"\n",
    "    Trains the model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        n_epochs: number of epochs\n",
    "        model: LanguageModel object\n",
    "        train_dl: training data\n",
    "        loss_fn: the loss function\n",
    "        opt_fn: the optimizer\n",
    "        lr: learning rate\n",
    "    \n",
    "    Returns:\n",
    "        The trained model. \n",
    "        A tuple of (model, train_losses, val_losses, train_perplexity, val_perplexity)\n",
    "    \"\"\"\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, batch in enumerate(getBatch(batch_size, train_data)):\n",
    "            inputs, targets = zip(*batch)\n",
    "            inputs = torch.cat(inputs) # B x 1\n",
    "            targets = torch.cat(targets) # B x 1\n",
    "            model.zero_grad()\n",
    "            \n",
    "            loss = model(inputs, targets, outers)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.data.tolist()[0])\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch : %d, mean_loss : %.02f\" % (epoch,np.mean(losses)))\n",
    "            losses = []\n",
    "    \n",
    "    return model, losses\n",
    "\n",
    "\n",
    "num_epochs = 1  # Max number of training epochs\n",
    "batch_size = 128  # Set the batch size\n",
    "embed_size = 10\n",
    "model = Skipgram(583, embed_size)\n",
    "lr = 0.1 # Set the learning rate\n",
    "opt_fn = optim.SGD(model.parameters(),lr=lr)  # Select an optimizer\n",
    "train_model(num_epochs, model, batch_size, train_data, opt_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7-general",
   "language": "python",
   "name": "py3.7-general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
